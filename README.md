# Chinese Prompt Optimizer

> **Reduce LLM token costs** by translating English system prompts into Chinese
> (token-dense), calling any LLM provider via LiteLLM, and translating the
> response back to English with NLP-based machine translation â€” all without
> losing contextual meaning.

---

## How it works

```
English system prompt
       â”‚
       â–¼  NLP translation (deep-translator / Google NMT)
       â”‚  + placeholder-based glossary to preserve technical terms
       â–¼
Chinese system prompt  â”€â”€â–¶  LiteLLM  â”€â”€â–¶  ChatGPT / Claude / Gemini
                                                    â”‚
                                                    â–¼
                                         Chinese response
                                                    â”‚
                                  NLP back-translation to English
                                                    â”‚
                                                    â–¼
                                           English answer
```

Chinese encodes more information per token than English: a typical system
prompt of 45 English tokens compresses to ~18 Chinese tokens â€” a **â‰ˆ 60 %**
saving on every API call.

---

## Similar projects

| Project | What it does |
|---------|-------------|
| [anomalyco/opencode](https://github.com/anomalyco/opencode) | Open-source AI coding agent; **provider-switching architecture** inspired our `providers.py` registry |
| [wyne1/llm-orchestrator](https://github.com/wyne1/llm-orchestrator) | Adapter-pattern LLM orchestrator for OpenAI / Anthropic / Gemini |
| [BerriAI/litellm](https://github.com/BerriAI/litellm) | Universal LLM gateway (used as our completion backend) |
| [nidhaloff/deep-translator](https://github.com/nidhaloff/deep-translator) | NLP translation library wrapping Google NMT (used for Englishâ†”Chinese) |

---

## Quick start

```bash
pip install -r requirements.txt
```

### CLI / script

```python
from chinese_prompt_optimizer import ChinesePromptOptimizer
import os

optimizer = ChinesePromptOptimizer(
    model="gemini/gemini-2.0-flash",          # or gpt-4o, anthropic/claude-3-5-sonnet-20241022
    api_key=os.environ["GEMINI_API_KEY"],
    glossary={
        "HIPAA": "HIPAA",      # keep acronym unchanged
        "LiteLLM": "LiteLLM", # keep brand name
    },
)

result = optimizer.complete(
    system_prompt="You are a helpful medical assistant. Always reference HIPAA.",
    user_message="What should I know about patient data privacy?",
    return_savings=True,
)

print(result["response"])
print(result["savings"])
# {'english_tokens': 18, 'chinese_tokens': 7, 'tokens_saved': 11, 'saving_pct': 61.11}
```

### GUI (Tkinter)

```bash
python -m chinese_prompt_optimizer.gui
```

The GUI lets you:
- **Switch providers** (ChatGPT / Claude / Gemini) from a dropdown
- **Switch models** within each provider
- **Enter API key** directly or set the env var (`GEMINI_API_KEY`, `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`)
- **Enter glossary terms** (`HIPAA=HIPAA`, `LiteLLM=LiteLLM`) to protect contextual meaning
- **View a live token line graph** showing English tokens, Chinese tokens (actual used), and saved tokens per run

---

## Provider support (opencode-style registry)

| Provider | Env var | Default model |
|----------|---------|---------------|
| **ChatGPT** (OpenAI) | `OPENAI_API_KEY` | `gpt-4o` |
| **Claude** (Anthropic) | `ANTHROPIC_API_KEY` | `claude-3-5-sonnet-20241022` |
| **Gemini** (Google AI Studio) | `GEMINI_API_KEY` | `gemini/gemini-2.0-flash` |

```python
from chinese_prompt_optimizer import get_provider, list_providers

for p in list_providers():
    print(p.name, "â†’", p.default_model)
```

---

## Context preservation

Technical terms, proper nouns, and domain jargon are **never** passed
through the NMT engine.  Supply a `glossary` dict and they will be swapped
with opaque placeholders before translation, then restored afterwards:

```python
optimizer = ChinesePromptOptimizer(
    model="gemini/gemini-2.0-flash",
    api_key=os.environ["GEMINI_API_KEY"],
    glossary={
        "HIPAA":   "HIPAA",          # keep unchanged
        "GPT-4o":  "GPT-4o",         # keep unchanged
        "RAG":     "æ£€ç´¢å¢å¼ºç”Ÿæˆ",      # force a specific Chinese term
    },
)
```

Long prompts are translated **sentence-by-sentence** so coherence is
maintained across clause boundaries.

---

## Token line graph

```python
from chinese_prompt_optimizer import token_savings_report, plot_token_comparison

reports = [
    token_savings_report("You are helpful.", "ä½ å¾ˆæœ‰å¸®åŠ©ã€‚"),
    token_savings_report(
        "You are a professional medical assistant. Always be accurate.",
        "ä½ æ˜¯ä¸“ä¸šçš„åŒ»ç–—åŠ©ç†ã€‚å§‹ç»ˆå‡†ç¡®ã€‚",
    ),
]
plot_token_comparison(reports, labels=["Short", "Long"], save_path="savings.png")
```

The graph shows three lines:
- ğŸ”µ **English tokens** (original)
- ğŸŸ¢ **Chinese tokens** (actual used)
- ğŸ”´ **Saved tokens**

with a shaded savings area between the two main lines.

---

## Running tests

```bash
# All unit tests (no API key needed)
pytest tests/ -v

# Live Gemini integration tests
GEMINI_API_KEY="AIza..." pytest tests/test_integration_gemini.py -v
```

The 5 integration tests in `test_integration_gemini.py` are automatically
**skipped** when `GEMINI_API_KEY` is not set or the network is unreachable â€”
they will run automatically in environments where both are available.

---

## Project structure

```
chinese_prompt_optimizer/
â”œâ”€â”€ __init__.py       â€“ public exports
â”œâ”€â”€ providers.py      â€“ opencode-style provider registry (ChatGPT/Claude/Gemini)
â”œâ”€â”€ translator.py     â€“ NLP translation with glossary & sentence chunking
â”œâ”€â”€ optimizer.py      â€“ ChinesePromptOptimizer (LiteLLM backend)
â”œâ”€â”€ utils.py          â€“ token counting + plot_token_comparison()
â””â”€â”€ gui.py            â€“ Tkinter GUI with embedded matplotlib line graph

tests/
â”œâ”€â”€ test_providers.py           â€“ provider registry unit tests
â”œâ”€â”€ test_translator.py          â€“ translation + context preservation tests
â”œâ”€â”€ test_optimizer.py           â€“ optimizer unit tests
â”œâ”€â”€ test_utils.py               â€“ token counting + graph tests
â””â”€â”€ test_integration_gemini.py  â€“ live Gemini integration tests (auto-skipped offline)

example.py    â€“ CLI demo (token savings report + optional live call)
```
